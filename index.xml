<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yuxing Wang</title>
    <link>https://Yuxing-Wang-THU.github.io/</link>
      <atom:link href="https://Yuxing-Wang-THU.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Yuxing Wang</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2023 Yuxing Wang</copyright><lastBuildDate>Sat, 01 Oct 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://Yuxing-Wang-THU.github.io/media/icon_hu03acb205fb0b40f635b48bbdfaf3118f_242332_512x512_fill_lanczos_center_3.png</url>
      <title>Yuxing Wang</title>
      <link>https://Yuxing-Wang-THU.github.io/</link>
    </image>
    
    <item>
      <title>Curriculum-based Co-design of Morphology and Control of Voxel-based Soft Robots</title>
      <link>https://Yuxing-Wang-THU.github.io/publication/cuco/</link>
      <pubDate>Sat, 01 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://Yuxing-Wang-THU.github.io/publication/cuco/</guid>
      <description>&lt;p style=&#34;text-align:justify&#34;;&gt;The philosophy of embodied cognition inspires the domain of robotics that a robot&#39;s ability to interact with the environment depends both on its brain (control policy) and body (morphology), which are inherently coupled. However, finding an optimal robot morphology and its controller for solving a given task is often unfeasible. The major challenge for this endeavor is the enormous combined design and policy space. Firstly, the freedom to pick the number of multi-material modules and the ways they are connected makes it notoriously difficult to explore the design space. For instance, in a robot simulator, there are over $4\times10^{8}$ possible morphologies for a robot composed of only 12 modules. Secondly, the evaluation of a morphology requires a separate training procedure for its unique controller.&lt;/p&gt;
&lt;p style=&#34;text-align:justify&#34;;&gt;In this work, we consider the co-optimization of design and control of Voxel-based Soft Robots (VSRs), a form of modular soft robots composed of elastic, multi-material cubic blocks. Unlike fragile fully-integrated robots, they can be easily disassembled and reassembled to adapt to a wide range of environments. For efficiently exploring the modular robot design space, prior approaches commonly rely on artificial evolution, which maintains a population of design prototypes and adopts a bi-level optimization loop, where the outer loop of morphology optimization is based on the fitness of individual controllers from the inner loop. These methods, however, tend to learn from scratch in the target design space where there is a significant combinatorial explosion. Thus, they spend a large amount of time on policy optimization and evaluation. Additionally, their separate training procedures significantly hinder the experience of design and control to be shared across different robots.&lt;/p&gt;
&lt;p style=&#34;text-align:justify&#34;;&gt;In view of these challenges, we propose a Curriculum-based Co-design (CuCo) method for learning to design and control VSRs from easy to difficult. CuCo draws inspiration from Curriculum Learning (CL) that starting with easier tasks can help the agent learn better when presented with more difficult tasks later on. The key to our approach is expanding the design space from a small size to the target size gradually through a predefined curriculum. Precisely, at each stage of the curriculum, we learn practical design and control patterns via Reinforcement Learning (RL), which is enabled by incorporating the design process into the environment and using differentiable policy representations. The converged morphology and the learned policies from last stage are inherited and then serve as the starting point for the next stage. Due to the exploitation of the previous knowledge of design and control, CuCo can quickly produce robust morphologies with better performance in larger dimensional settings.&lt;/p&gt;
&lt;p style=&#34;text-align:justify&#34;;&gt; Visualization of converged morphologies of all methods in the Pusher environment across three different runs. Below each VSR is its average performance.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./CuCo2.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p style=&#34;text-align:justify&#34;;&gt; Visualization of CuCo’s learning process in UpStepper and Pusher. The x-axis represents the number of policy iterations and the y-axis represents the curriculum. We show morphologies in the stage from left to right.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./CuCo3.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;More visual results are shown in the following video.&lt;/p&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://Yuxing-Wang-THU.github.io/media/video.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
</description>
    </item>
    
    <item>
      <title>ModularEvoGym</title>
      <link>https://Yuxing-Wang-THU.github.io/project/modularevogym/</link>
      <pubDate>Tue, 27 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://Yuxing-Wang-THU.github.io/project/modularevogym/</guid>
      <description>&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./robot.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p style=&#34;text-align:justify&#34;;&gt; ModularEvoGym [1] is based on Evolution Gym [2], a large-scale benchmark for co-optimizing the design and control of Voxel-based Soft Robots (VSRs). We modified the original state representation to formulate a new modular observation space. The input state of the robot at time step $t$ is represented as $s_{t}^{c}=\lbrace s_{t}^{v},s_{t}^{g}\rbrace$, where $s_{t}^{v}=\lbrace s_{t}^{v_{1}}, s_{t}^{v_{2}},...,s_{t}^{v_N}\rbrace$, $s_{t}^{v_i}$ is composed of each voxel&#39;s local information which contains the relative position of its four corners with respect to the center of mass of the robot and its material information (e.g., &lt;b&gt;&lt;font color=Gray&gt;soft voxel&lt;/font&gt;&lt;/b&gt;, &lt;b&gt;rigid voxel&lt;/b&gt;, &lt;b&gt;&lt;font color=Darkorange&gt;horizontal actuator&lt;/font&gt;&lt;/b&gt; and &lt;b&gt;&lt;font color=DeepSkyBlue&gt;vertical actuator&lt;/font&gt;&lt;/b&gt;). $s_{t}^{g}$ is the task-related observation such as terrain information of the environment and goal-relevant information. During the simulation, voxels (except empty voxels) only sense locally, and based on the input sensory information, a controller outputs control signals to vary the volume of actuator voxels. The morphology of the robot is unchangeable during the interaction with the environment. &lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./robot2.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p style=&#34;text-align:justify&#34;;&gt; With the support of ModularEvoGym, we can do many interesting things. We modeled the local observations of all voxels as a sequence and adopted the self-attention mechanism [3] to develop a more efficient controller (shown below) that handles incompatible state-action spaces. This controller can be trained by popular Reinforcement Learning algorithms (e.g., PPO [4]) to simultaneously control a variety of robot morphologies.&lt;/p&gt; 
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./controller.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p style=&#34;text-align:justify&#34;;&gt; For encoding a VSR&#39;s morphology, apart from using methods such as direct encoding and Compositional Pattern Producing Network (CPPN) [5], which rely on having access to the whole design space, we provided another choice, Neural Cellular Automata (NCA) [6], which takes multiple actions to grow a robot from an initial seed (morphology), as shown below. NCA encodes complex patterns in a neural network and generates different developmental outcomes while using a smaller set of trainable parameters.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./design.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Built upon the aforementioned successes, we also presented an efficient Curriculum-based Co-design (CuCo) method for learning to design and control VSRs through an easy-to-difficult process [7]. The following pictures show a developmental process of a walker agent. For more details, please refer to &lt;a href=&#34;https://yuxing-wang-thu.github.io/publication/cuco/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CuCo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;$3 \times 3$ design space, $9$ voxels.
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./1.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

$5 \times 5$ design space, $25$ voxels.
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./2.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

$7 \times 7$ design space, $49$ voxels.
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./3.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

$9 \times 9$ design space, $81$ voxels.
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./4.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

$11 \times 11$ design space, $121$ voxels.
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./5.gif&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;[1] &lt;a href=&#34;https://github.com/Yuxing-Wang-THU/ModularEvoGym&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ModularEvoGym&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] &lt;a href=&#34;https://evolutiongym.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Evolution Gym: A Large-Scale Benchmark for Evolving Soft Robots&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Attention Is All You Need&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] &lt;a href=&#34;https://arxiv.org/abs/1707.06347&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Proximal Policy Optimization Algorithms&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5] &lt;a href=&#34;https://link.springer.com/article/10.1007/s10710-007-9028-8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Compositional Pattern Producing Network&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[6] &lt;a href=&#34;https://distill.pub/2020/growing-ca/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neural Cellular Automata&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[7] &lt;a href=&#34;https://yuxing-wang-thu.github.io/publication/cuco/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Curriculum-based Co-design of Morphology and Control of Voxel-based Soft Robots&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dynamics-Adaptive Continual Reinforcement Learning via Progressive Contextualization</title>
      <link>https://Yuxing-Wang-THU.github.io/publication/dacorl/</link>
      <pubDate>Wed, 04 May 2022 00:00:00 +0000</pubDate>
      <guid>https://Yuxing-Wang-THU.github.io/publication/dacorl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Surrogate-Assisted Controller for Expensive Evolutionary Reinforcement Learning</title>
      <link>https://Yuxing-Wang-THU.github.io/publication/serl/</link>
      <pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://Yuxing-Wang-THU.github.io/publication/serl/</guid>
      <description>&lt;p style=&#34;text-align:justify&#34;;&gt;A HalfCheetah agent trained by SPDERL-I with average performance of 14000 points over 50 test seeds. The agent is able to adjust its posture more appropriately and run faster.&lt;/p&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://Yuxing-Wang-THU.github.io/media/2.HalfCheetah_trained_by_SPDERL-I_14000.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p style=&#34;text-align:justify&#34;;&gt;A Hopper agent trained by SPDERL-I with average performance of 4100 points over 50 test seeds. The agent jumps faster and learns to better stabilize the center of gravity.&lt;/p&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://Yuxing-Wang-THU.github.io/media/3.Hopper_trained_by_SPDERL-I_4100.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p style=&#34;text-align:justify&#34;;&gt;A Walker agent trained by SPDERL-G with average performance of 9000 points over 50 test seeds. The agent learns to walk with only one leg and use the other leg to maintain balance.&lt;/p&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://Yuxing-Wang-THU.github.io/media/6.Walker_trained_by_SPDERL-G_9000.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
</description>
    </item>
    
    <item>
      <title>From Big Data Based Price Discrimination to Privacy Leakage: Ethical Analysis and Reflections on Privacy Issues from the Perspective of Hardware and Software</title>
      <link>https://Yuxing-Wang-THU.github.io/publication/dataethics/</link>
      <pubDate>Sun, 03 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://Yuxing-Wang-THU.github.io/publication/dataethics/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Evolutionary Arts</title>
      <link>https://Yuxing-Wang-THU.github.io/project/evolutionary_arts/</link>
      <pubDate>Wed, 03 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://Yuxing-Wang-THU.github.io/project/evolutionary_arts/</guid>
      <description>&lt;p style=&#34;text-align:justify&#34;;&gt;Using Compositional Pattern-Producing Network(CPPN) to generate pictures, very funny.&lt;/p&gt;
&lt;p&gt;Here is the &lt;a href=&#34;https://github.com/Yuxing-Wang-THU/Evolutionary-Arts&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/evolutionary_arts/a_hu10332d6ec3a754b8b8cfac83b926b1f2_26704_a55a3e289d74dd219c0bc5193d382472.jpg 400w,
               /project/evolutionary_arts/a_hu10332d6ec3a754b8b8cfac83b926b1f2_26704_d5a85f11e81fa636fb05230119bea761.jpg 760w,
               /project/evolutionary_arts/a_hu10332d6ec3a754b8b8cfac83b926b1f2_26704_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://Yuxing-Wang-THU.github.io/project/evolutionary_arts/a_hu10332d6ec3a754b8b8cfac83b926b1f2_26704_a55a3e289d74dd219c0bc5193d382472.jpg&#34;
               width=&#34;760&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/evolutionary_arts/b_hu39c428bcc29877a80a46cb34eac21bae_38293_4cb07847623e14b6652dbdd41f8828b5.jpg 400w,
               /project/evolutionary_arts/b_hu39c428bcc29877a80a46cb34eac21bae_38293_0308405e7647a05f38a4f4d765e48546.jpg 760w,
               /project/evolutionary_arts/b_hu39c428bcc29877a80a46cb34eac21bae_38293_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://Yuxing-Wang-THU.github.io/project/evolutionary_arts/b_hu39c428bcc29877a80a46cb34eac21bae_38293_4cb07847623e14b6652dbdd41f8828b5.jpg&#34;
               width=&#34;760&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/evolutionary_arts/c_hu28bee01178a13f6a332b6bab82fe4320_40159_ab2ad65a0b999e2b975c3e20f7eab211.jpg 400w,
               /project/evolutionary_arts/c_hu28bee01178a13f6a332b6bab82fe4320_40159_b97c77ac052d61906e3c1e404a039f2d.jpg 760w,
               /project/evolutionary_arts/c_hu28bee01178a13f6a332b6bab82fe4320_40159_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://Yuxing-Wang-THU.github.io/project/evolutionary_arts/c_hu28bee01178a13f6a332b6bab82fe4320_40159_ab2ad65a0b999e2b975c3e20f7eab211.jpg&#34;
               width=&#34;760&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/evolutionary_arts/d_hu656cc3699bc6a25c8e0dea527ac50132_106737_28dd5536ec9e84337563c8970bd68b3e.jpg 400w,
               /project/evolutionary_arts/d_hu656cc3699bc6a25c8e0dea527ac50132_106737_79923bd42492c8736b615cbdd6e95cd6.jpg 760w,
               /project/evolutionary_arts/d_hu656cc3699bc6a25c8e0dea527ac50132_106737_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://Yuxing-Wang-THU.github.io/project/evolutionary_arts/d_hu656cc3699bc6a25c8e0dea527ac50132_106737_28dd5536ec9e84337563c8970bd68b3e.jpg&#34;
               width=&#34;760&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/evolutionary_arts/e_hu2568418f64e513215520ef1ac53d0693_113338_5092de113907a9f459db65e337f383b0.jpg 400w,
               /project/evolutionary_arts/e_hu2568418f64e513215520ef1ac53d0693_113338_30581c73e35ad90414354ed434c68124.jpg 760w,
               /project/evolutionary_arts/e_hu2568418f64e513215520ef1ac53d0693_113338_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://Yuxing-Wang-THU.github.io/project/evolutionary_arts/e_hu2568418f64e513215520ef1ac53d0693_113338_5092de113907a9f459db65e337f383b0.jpg&#34;
               width=&#34;760&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/evolutionary_arts/f_hufd03d26cfd97fa27e8837c25d1fbbb5e_148623_8f446c048d8088a0e67466a53613d08f.jpg 400w,
               /project/evolutionary_arts/f_hufd03d26cfd97fa27e8837c25d1fbbb5e_148623_09d1257d318c2445f6e991a58ad01b4a.jpg 760w,
               /project/evolutionary_arts/f_hufd03d26cfd97fa27e8837c25d1fbbb5e_148623_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://Yuxing-Wang-THU.github.io/project/evolutionary_arts/f_hufd03d26cfd97fa27e8837c25d1fbbb5e_148623_8f446c048d8088a0e67466a53613d08f.jpg&#34;
               width=&#34;760&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/evolutionary_arts/g_hue4f9ebd7b3f226ba07b620bdec68bafc_182563_d5222d556a59a30f912c52dcd9ba8adc.jpg 400w,
               /project/evolutionary_arts/g_hue4f9ebd7b3f226ba07b620bdec68bafc_182563_be476563b1e4ab398389aeadf12aa948.jpg 760w,
               /project/evolutionary_arts/g_hue4f9ebd7b3f226ba07b620bdec68bafc_182563_1200x1200_fit_q75_lanczos.jpg 1200w&#34;
               src=&#34;https://Yuxing-Wang-THU.github.io/project/evolutionary_arts/g_hue4f9ebd7b3f226ba07b620bdec68bafc_182563_d5222d556a59a30f912c52dcd9ba8adc.jpg&#34;
               width=&#34;760&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>International Underwater Robot Competition</title>
      <link>https://Yuxing-Wang-THU.github.io/project/fish/</link>
      <pubDate>Thu, 17 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://Yuxing-Wang-THU.github.io/project/fish/</guid>
      <description>&lt;p style=&#34;text-align:justify&#34;;&gt;The International Underwater Robot Competition is an international contest sponsored by the International League of Underwater Robots (ILUR) supported by both the UNESCO Industry-University Cooperative Education &amp; the Ministry of Education’s Innovative Education Method Steering Committee.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./ff.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p style=&#34;text-align:justify&#34;;&gt;In the 2D Simulation League, teams of autonomous software programs play in a two-dimensional virtual playground represented by a central server, called URWPGSim2DServer. This server knows everything about the game, i.e. the current position of all fishes and the physics. The game further relies on communication between the server and each agent. Each fish receives relative and noisy input from its virtual sensors and performs some basic commands to influence its environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Survival Challenge&lt;/strong&gt;&lt;/p&gt;
&lt;p style=&#34;text-align:justify&#34;;&gt;In this contest, we first control the light blue simulated fish to attack the yellow simulated fishes controlled by the defender (the other team), while the defender can also use a dark blue simulated fish to block the attacker&#39;s attack. The game changes the side after 5 minutes. We won the first prize in IURC 2017 and IURC 2018.&lt;/p&gt;
&lt;p&gt;A short video&lt;/p&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://Yuxing-Wang-THU.github.io/media/catcher.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p&gt;&lt;strong&gt;Artistic Swimming&lt;/strong&gt;&lt;/p&gt;
&lt;p style=&#34;text-align:justify&#34;;&gt;In this contest, ten red simulated fishes are controlled by each team to perform artistic swimming and one yellow simulated fish is controlled by the server to disrupt other fishes.&lt;/p&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://Yuxing-Wang-THU.github.io/media/swimmer.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
</description>
    </item>
    
    <item>
      <title>A wearable hand rehabilitation robot</title>
      <link>https://Yuxing-Wang-THU.github.io/project/hand/</link>
      <pubDate>Thu, 25 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://Yuxing-Wang-THU.github.io/project/hand/</guid>
      <description>&lt;p style=&#34;text-align:justify&#34;;&gt; In this project, we design a wearable hand rehabilitation robot, which consists of a data glove, a manipulator and an APP. This system allows the patient to use a healthy hand to operate the rehabilitation glove, thus controlling the robot to drive the disabled hand to reproduce its movements. At the same time, combined with the rehabilitation program customized by professional physicians integrated into the APP, the system can provide corresponding rehabilitation training for different degrees of disability.&lt;/p&gt;
&lt;p&gt;A short vedio&lt;/p&gt;









  





&lt;video controls  &gt;
  &lt;source src=&#34;https://Yuxing-Wang-THU.github.io/media/my_video.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://Yuxing-Wang-THU.github.io/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://Yuxing-Wang-THU.github.io/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://owchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
Three
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/discussions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://Yuxing-Wang-THU.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://Yuxing-Wang-THU.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
